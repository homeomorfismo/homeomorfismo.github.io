---
layout: post
title: "Adaptative procedural for computing eigenvalues II"
description: "Discussion of the implementation of different approaches of error adaptivity methods for solving elliptic eigenvalue problems."
comments: false
keywords: "eigenvaluse, finite, elements, adaptivity"
---

# General setting

Recall that, for a given differential operator \\( \mathcal{A} \\) that is uniformly elliptic on a bounded open domain \\( \Omega \subset \mathbb{R}^d\\), we formulate the classical eigenvalue problem.
This means, we want to find \\( v \\) and \\( \lambda \\) such that 
\\[ \mathcal{A} v = \lambda v, \text{ in } \Omega, \ \ v=0, \text{ on } \partial \Omega. \\]

Consider the variational formulation of the previous problem. I.e., we seek \\( (v,\lambda) \in H\times \mathbb{C} \\) such that
\\[ a(v,w) = \lambda (v,w), \ \ \forall v \in H, \ \lVert v \rVert = 1. \\]

Define \\( V:=H\times \mathbb{C} \\). Then, we can summarize both constrains into the following expression
\\[ A(v,\lambda; w, \chi) := -a(v,w) + \lambda(v,w) + \overline{\chi}(\lVert v \rVert^2 - 1),\\]
for all \\( (w,\chi) \in \mathbb{C}\\).

# Optimizing our goal

We still aim to optimize the quantity
\\[ j_\omega(u) := \frac{1}{2\imath  \lVert u\rVert_\omega^2} \int_{\partial \omega} (\partial_n \overline{u})u - (\partial_n u)\overline{u} \, \mathrm{d}s. \\]
For general purposes, we use \\( J : V \to \mathbb{R} \\).
We might need to study the posibility of regularizing \\( j_\omega\\).

## Euler-Lagrange approach

Defining a Lagrangian and taking into account the Euler-Lagrange equations, we obtain two sets of equations.
The primal problem is defined by our *primal* problem which is defined by the constrain and is equivalent to , i.e.,
\\[ A(v,\lambda; w, \chi) := -a(v,w) + \lambda(v,w) + \overline{\chi}(\lVert v \rVert^2 - 1) = 0, \\]
for all \\( (w,\chi) \in \mathbb{C}\\). 

The dual problem is defined by the second optimality condition.
\\[ \dot{A}(v,\lambda; \hat{v}, \hat{\lambda}, w, \chi) = \dot{J}(v,\lambda; \hat{v}, \hat{\lambda}). \\]
where 
\\[ \dot{A}(v,\lambda; \hat{v}, \hat{\lambda}, w, \chi)
= -a(\hat{v},w) + \lambda(\hat{v},w) + \hat{\lambda}(v,w) + 2\chi\Re(v,\hat{v}).\\]

# Big hammer: a posteriori error representation

For a general constrained problem of the form \\( A(u;f) = F(f) \\) and a functional \\(J\\), the error given by the Galerkin approximation of the saddle-point appoximation is given by 
\\[ J(u) - J(u_h) = \frac{1}{2}\rho(u_h;z - \tilde{z_h}) + \frac{1}{2}\rho^\star(z_h;u - \tilde{u_h}) + R, \\]
where \\( R\\) is a third-order residual.

# Morals of the technique

For some carefully selected \\( J\\), the residual term vanishes.
We can construct different error estimators after breaking down the weak forms, and applying several Cauchy-Schwarz inequalities.
I hope to show you some other examples of this technique!
